services:
  cu124_torch25_base:
    build:
      context: .
      dockerfile: cu124_torch25_base/Dockerfile
      args:
        # pass through env vars to docker build
        DOCKER_USER: $DOCKER_USER
        HF_TOKEN: $HF_TOKEN
        USERNAME: $USER
    image: cu124_torch25_base:latest

  internvl2-8b:
    build:
      context: .
      dockerfile: internvl2-8b/Dockerfile
      args:
        GIT_DOMAIN: huggingface.co
        GIT_ORG: OpenGVLab
        GIT_REPO: InternVL2-8B
        GIT_BRANCH: main
        DIR: internvl2-8b
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: internvl2-8b
    depends_on:
      - cu124_torch25_base
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true

  ovis1.6-gemma2-9b:
    build:
      context: .
      dockerfile: ovis1.6-gemma2-9b/Dockerfile
      args:
        GIT_DOMAIN: huggingface.co
        GIT_ORG: AIDC-AI
        GIT_REPO: Ovis1.6-Gemma2-9B-GPTQ-Int4
        GIT_BRANCH: main
        DIR: ovis1.6-gemma2-9b
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: ovis1.6-gemma2-9b
    depends_on:
      - cu124_torch25_base
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true

  instructblip-vicuna-7b:
    build:
      context: .
      dockerfile: instructblip-vicuna-7b/Dockerfile
      args:
        GIT_DOMAIN: huggingface.co
        GIT_ORG: Salesforce
        GIT_REPO: instructblip-vicuna-7b
        GIT_BRANCH: main
        DIR: instructblip-vicuna-7b
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: instructblip-vicuna-7b
    depends_on:
      - cu124_torch25_base
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true

  salesforce_blip2:
    build:
      context: .
      dockerfile: Salesforce/blip2/Dockerfile
      args:
        GIT_DOMAIN: huggingface.co
        GIT_ORG: Salesforce
        GIT_REPO: blip2-opt-6.7b-coco
        GIT_BRANCH: main
        DIR: Salesforce/blip2
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: salesforce_blip2
    depends_on:
      - cu124_torch25_base
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true
  # Qwen/Qwen2-VL-7B-Instruct
  qwen2_vl_7b_instruct:
    build:
      context: .
      dockerfile: qwen/qwen2-vl/Dockerfile
      args:
        GIT_DOMAIN: huggingface.co
        GIT_ORG: Qwen
        GIT_REPO: Qwen2-VL-7B-Instruct
        GIT_BRANCH: main
        DIR: qwen/qwen2-vl/
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: qwen2_vl_7b_instruct
    depends_on:
      - cu124_torch25_base
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true
  # openbmb/MiniCPM-V-2_6
  minicpm-v-2_6:
    build:
      context: .
      dockerfile: openbmb/minicpm-v-2_6/Dockerfile
      args:
        GIT_DOMAIN: huggingface.co
        GIT_ORG: openbmb
        GIT_REPO: MiniCPM-V-2_6
        GIT_BRANCH: main
        DIR: openbmb/minicpm-v-2_6/
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    environment:
      - HF_TOKEN=${HF_TOKEN}
    image: minicpm-v-2_6
    depends_on:
      - cu124_torch25_base
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true
